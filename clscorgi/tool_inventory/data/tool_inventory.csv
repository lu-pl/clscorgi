 ,toolname,alternate name,what can this tool do for you?,primary_purpose_consolidated_CLSCor_vocab,Language_consolidated,Language_comment,Version_consolidated,Version_comment,Version Date_consolidated,Version Date_comment,Works on Operating Systems (consolidated CLSCor vocab),Works on Operating Systems_comment,License (consolidated CLSCor vocab),License_comment,Distribution,User interface,How does the tool process your text,output_format (consolidated CLSCor vocab),output_format_comment,statistical_models,input_format (consolidated CLSCor vocab),input_format_comment,Which other tools from this list does this tool integrate?,metric,visualisation,formalism,tagset
1,Arborator Grew,,"Arborator Grew is an interactive and user-friendly on-line tool for annotating treebanks. It provides annotation management over several users with different editing rights. It can also be used in the classroom. The teacher can provide correct solutions and make them visible for the students to a varying extent.  
Functionalities: query language + writing functions, collaborative dependency annotation ","annotating, applying NLP methods",language-agnostic,Any (it is language-agnostic),unknown,,unknown,,"Linux, MacOS, Windows,  Unix",all,Copyright Not Evaluated,Fully free,"web service, local server with client integrated in the web browser",web browser,it displays,,,"This tool is not a statistical model / a set of statistical models, and it does not include statistical models.",CoNLL-U,,,,,Grew syntax,
2,conll-u editor,,"This tool allows you to manually create or edit corpora annotated with Universal Dependencies. It provides a particularly user-friendly tokenization editing, that is, it is very easy to delete, split or merge tokens or sentences – a feature that is not automatically provided with annotation tools! Also, the sentences can be visualized as trees or flat graphs or tables. ",annotating,language-agnostic,Any (it is language-agnostic),v2.20.0,,2023-01-22,,Linux,"the server runs on Linux, but the annotation runs in web browsers across operating systems.",BSD 3-Clause License,Fully free (BSD 3-Clause License),local client-server system,web browser frontend,It loads a CoNLL-U file and allows you to edit it. ,CoNLL-U,,"This tool is not a statistical model a set of statistical models, and it does not include statistical models.",CoNLL-U,,,,,Universal Dependencies,Universal Dependencies
3,INCEpTION,,"This tool lets you to manually add markup to your documents. It can even learn from your supplied annotation and pre-annotate. Mind that the performance depends on data size and annotation complexity (e. g. the number of labels). However, it is a high-quality tool even without considering the pre-annotation option at all. ",annotating,language-agnostic,Any (it is language-agnostic),v27.0,,2023-03-07,,"Linux, MacOS, Windows",,Apache Software License 2.0,Apache 2.0,".jar file, local server installation with client opening in a browser window (Chrome, Safari, Firefox)",client in the web browser,"It reads in your tool, allows adding markup according to templates (defined by you or pre-defined) and stores it.",TEI,although issues may arise when it should preserve some previously added markup,"This tool is (not) a statistical model / a set of statistical models, but it does include statistical models.
You can easily train your statistical model to plug in this tool. ","CoNLL 2000, CoNLL2002, CoNLL 2003, CoNLL 2006, CoNLL 2009, CoNLL 2012, CoNLL CoreNLP, IMS CWB VRT, ConLL-U, HTML, NIF, PDF, PERSEUS_2.1, TCF, TEI P5, TXT, TSV, XML",This tool accepts many formats. See https://inception-project.github.io/releases/27.0/docs/user-guide.html#sect_formats,,,,user dependent,user dependent
4,DraCor,,"DraCor simplifies accessing specific parts of a TEI annotated dramatic texts (such as the spoken text per character, spoken text of characters by gender, stage directions, etc.) DraCor automatically extracts textual features (e. g. number of characters, speeches, scenes, etc.), generates networks based on co-occurrences of characters in a scene, and calculates network metrics. The derived data can be exported in various formats, e.g., JSON, CSV, RDF but also specialized formats, e.g., GEXF format to be used in Gephi, a tool for network analysis. The DraCor API offers numerous options and additional formats for further processing of the unified metadata. A local instance of the DraCor system can be set up using Docker, which can be used to work with custom corpora or in copyright material.","corpus management, applying textometry methods, 
information extraction",language-agnostic,Any (it is language-agnostic),v0.87.1,,2022-12-30,,"Windows, Unix, MacOS, Linux",,Copyright Not Evaluated,Fully free,Cloud service,"GUI, API-REST","Returns some metrics as numbers or tables. 
Returns some visualization (plots, interactive elements, etc.).","JSON, CSV, RDF, GEXF, TEI","The tool requires text in XML-TEI, adds its markup in XML-TEI, and ensures valid XML-TEI on the output. ","This tool is not a statistical model / a set of statistical models, and it does not include statistical models.",TEI,"Should conform to DraCor Schema (see https://github.com/dracor-org/dracor-schema/tree/main/odd). The tool requires text in XML-TEI, adds its markup in XML-TEI, and ensures valid XML-TEI on the output. ",,,,,
5,IMS Corpus Workbench,alias CWB,"The IMS Open Corpus Workbench is a tool for managing and querying text corpora. It is a prequel to other big corpus managers such as KonText, where the main difference is that CWB is just the server part (backend) that can be supplemented with a web-based GUI like CQPweb. ",corpus management,language-agnostic,Any (it is language-agnostic),v3.5,,unknown,,"Linux, MacOS, Windows",,Copyright Not Evaluated,Fully free,Local server-client installation,"GUI, API",Allows you to search through your texts and returns matching results.,,,"This tool is not a statistical model / a set of statistical models, and it does not include statistical models.","CoNLL-U, VERTICAL","The input text has to be transformed into a vertical, CoNLL-U-like format. ",,,,CQL,
6,Kontext,,"KonText is an advanced and customizable corpus query engine built above the Manatee library of NoSketch Engine. KonText is intended for professional use and deployment in organizations with many users, rather than for home use. Its main features are: full support for parallel corpora, partial support for syntactically annotated and spoken corpora. Manatee is a token-based backend that is fast and scalable, but its support for corpora with complex annotation (organized in multiple interfering layers) is limited.",corpus management,language-agnostic,Any (it is language-agnostic),v0.17,,2023-02,February 2023,Linux,,Copyright Not Evaluated,Fully free (GPL 2.0),Local server-client installation,"GUI, API","Allows you to search through your texts and returns matching results.
Returns some visualization (plots, interactive elements, etc.).
",,,"This tool is not a statistical model / a set of statistical models, and it does not include statistical models.","CoNLL-U, VERTICAL","The input text has to be transformed into a vertical, CoNLL-U-like format.",,"Statistics expressing the mutual attraction of two words or short text segments (MI, Log-Likelihood Ratio)",,CQL (Corpus Query Language),
7,SimpleCorp,,"SimpleCorp is primarily intended as a teaching tool for NLP courses. It allows students (or researchers) to create a corpus from a collection of documents, as long as they have an SSO login. And it tries to make that process as straightforward as possible. Upon upload, the tool converts the documents to TEI/XML, and then applies a pre-defined NLP pipeline. After uploading a collection of files, it will compile a searchable corpus in CWB from the collection of XML files. 
There currently is no docker instance, but once out of beta, we intend to create a docker instance","corpus management, applying NLP methods",language-agnostic,The tool can work with any language supported by the UDPipe REST service,v0.6,,2023-01-15,,"Linux, MacOS",,Copyright Not Evaluated,to be determined,Server-based tool,Web-based,"Drop-and-drop files.
Automatic conversion to TEI/XML (TEITOK style).
Automatic application of NLP pipeline.
Automatic creation of a searchable corpus.
","TEI, XML","The whole tool is TEI/XML based, with minimal deviation from the standard to make it possible to edit and process/display, but all such deviation can be converted to standard TEI in the export.","This tool is not a statistical model / a set of statistical models, but it does include statistical models.
SimpleCorp is a streamlined, but restricted instance of TEITOK, which does not itself allow any training, but only applies pre-defined NLP tools (currently UDPipe2). But the project itself can be exported to TEITOK, where tools can be trained.","TEI, XML","The drag-and-drop module converts from a number of different format, so that ideally no pre-structuring is necessary. But all internal files are TEI/XML. The whole tool is TEI/XML based, with minimal deviation from the standard to make it possible to edit and process/display, but all such deviation can be converted to standard TEI in the export.","udpipe, IMS Corpus WorkBench, TEITOK",,,"CQL, Universal Dependencies",Universal Dependencies
8,TEITOK,,"TEITOK can be used to create corpora and keep a wide range of different types of information (facsimile alignment, audio alignment, dependency relations, pos tagging, geolocation information, IGT, morphological data, parallel alignment, witness alignment). It then provides various interfaces to exploit all these types of information, and in most cases provides an editing environment as well to provide automatic annotations and manually correct or create annotations. It also provides searches through the corpus, either using CQL for simple annotated files, or Grew or PML-TQ for dependency parsed corpora. It also provides specific search functionality for parallel corpora (be it based on translation or on witnesses) as well as for spoken and facsimile aligned data. 
This tool is compatible with UDPipe, Corpus WorkBench, DraCor, NameTag, QuitaUp, INCePTION, GrewMatch, NeoTag.","corpus management, applying NLP methods, applying textometry methods, annotating, information extraction",language-agnostic,"The tool is fully language independent, and has support for right-to-left (RTL) languages, as well as non-alphabetic scripts. TEITOK has been often used for NLP on left-to-right (LRL) languages such as Ladino or Sardinian.",v3.2.781,,2023-12-29,,"Linux, MacOS",,Copyright Not Evaluated,Fully free (GNU GPLv3),Local client-server installation,"Web GUI, REST-API
","Convert from several input formats.
Run annotation tools on the server for automatic annotation.
Manually correct or annotate in the GUI.
Use the API to run processing or annotation tools remotely.","TEI, XML","The whole tool is TEI/XML based, with minimal deviation from the standard to make it possible to edit and process/display, but all such deviation can be converted to standard TEI in the export.","This tool is not a statistical model / a set of statistical models, but it does include statistical models. 
There is a native POS tagger (NeoTag) that can be easily trained in the GUI on the manually corrected data. But NeoTag is gradually being replaced by UDPIPE, which currently can only be trained in the GUI using the older 1.0 version.","TEI, XML","The input should ideally be in TEI/XML, but there are various conversion tools that convert from a collection of OCR, Spoken, and annotated formats. But all internal files are TEI/XML. The whole tool is TEI/XML based, with minimal deviation from the standard to make it possible to edit and process/display, but all such deviation can be converted to standard TEI in the export.","udpipe, IMS Corpus WorkBench, DraCor, NameTag 1, NameTag2, QuitaUp, INCePTION","Statistics expressing the mutual attraction of two words or short text segments (MI, Log-Likelihood Ratio).
Token frequencies, Lexical richness/diversity, Key words, Thematic concentration, Descriptivity/activity, Verb distance, Entropy, Hapaxes. 
Degree centrality, Eigenvector centrality, Weight",,CQL,"PMLTQ, Grew"
9,TXM desktop,,"TXM can be used to create and analyze four main types of textual corpora:
- written texts (possibly aligned with fac-simile).
- speech transcriptions (possibly synchronized with media files at word level).
- multilingual/parallel corpora.
- and corpora encoded in a table (e.g. answers to a survey, tweets).
TXM can automatically annotate texts with part of speech and lemma with the help of TreeTagger (or other NLP tools) or use linguistic annotations already encoded into the sources.

One specificity of TXM is that it can help analyse any kind of TEI encoded corpus by possibly adapting (with the help of an XSLT transformation stylesheets library) the texts encodings to its internal XML-TEI TXM format and data model if needed.

Another specificity of TXM is that it can help the user to process progressively his corpus from plain text to richly structured XML-TEI encoded text. It offers a continuous range of import modules covering the most frequently used standard formats:
- TXT: for any plain text coming from word processors, PDFs, websites, etc.
- XML: for slightly structured texts (only sentences or paragraphs for example) or linguistically enriched (with XML tags that encode certain words with lexical properties)
- TEI: for texts encoded according to the recommendations of the TEI consortium and which are intended to be capitalized for long-term projects, shared with other initiatives or compatible with archiving systems.

A project can apply TXM to its encoded data progressively, from the simplest way (and most limited to use) to the most complex way (and richest). TXM therefore makes it possible to adapt the costs of corpus encoding according to the real needs of the study, especially when these needs are discovered as the corpus is being analyzed. Under these conditions, TXM assists both the encoding activity and the exploitation of corpora.

When using TEI, TXM helps to build and host online rich HTML 5 + CSS 3 + Javascript based text editions, with possible fac-simile synoptic views (when page image links are simply encoded in <pb/> tags in the documents).

TXM supports various flavors of TEI P4/P5 encoding practices: Perseus, TextGrid, Base de Français Médiéval (BFM), BVH Epistemon, etc. TEI sources are preprocessed by several XSL stylesheets that are delivered with TXM. Other stylesheets are available in the TXM XSL stylesheets library online: http://sourceforge.net/projects/txm/files/library/xsl.

TXM provides four types of tools to analyze a corpus:
- Corpus configuration tools to build specific subcorpora or corpus partitions, based on sets of texts or sets of text structures (inside a text)
- Semi-manual annotation tools at word or word sequence level: through concordance or text reading
- Qualitative analytic tools like word pattern lists, concordances or progression views
- Quantitative analytic tools like word pattern cooccurrents analysis, keyword analysis, and contrastive tools like correspondence factorial analysis or cluster analysis

Every analytic tool uses word patterns expressed with search queries on annotations with the help of different search engines (Corpus Query Processor, TIGER Search or URSQL). For example, word patterns expressed with the CQL query language:
- ""aiming"": to simply search for the word 'aiming'
- [pos=""VERB"" & word="".*ing""]: to search for verb forms ending in "".ing"" (where Part of Speech annotation is present)
- [lemma=""group""] []{0,3} [pos=""VERB"" & word="".*ing""]: to search for the collocation <group lemma> followed by a <verb with progressive aspect> with at most 3 words in between

TXM makes an analysis working session more fluid by interconnecting the tools as much as possible through hypertextual links located in their result views. For example: a double-click from a density curve of a word pattern in a text (general view of Progression) to the Concordance of this pattern (focused contextual reading), then from a line of this concordance to an Edition page where the occurrence is highlighted (precise reading of the text), then from this page to the Playback of the video corresponding to the transcription at the time the occurrence is pronounced (verification in the primary source).

All statistical models used by the quantitative tools are implemented and documented in R.

Results can be exported as spreadsheets (for data) or images (for visualisations). Texts and annotations can be exported in XML-TEI format. Corpora can be exported as a .txm file to be loaded in another TXM or uploaded to a TXM portal server version for online access and analysis.

TXM tools can be driven by Groovy scripts (a Java based scripting language) or by R scripts.

This tool is compatible with: TreeTagger, UDPipe (prototype), CQP (Corpus Query Processor), CQP as a library version (https://gitlab.huma-num.fr/txm/txm-cqp), R textometry package (https://cran.r-project.org/web/packages/textometry/index.html), R FactoMineR package (https://cran.r-project.org/web/packages/FactoMineR/index.html)","corpus management, corpus extraction, corpus analysis, applying NLP methods, annotating",language-agnostic,"The tool is fully language independent: any Unicode script, any character collation.
Automatic linguistic annotations depend on NLP tools used or annotations imported from source.
For example, TreeTagger can lemmatize English, German, Italian, Spanish, Russian, Latin, Greek, Ancien Greek, Written Modern French, Spoken Modern French, Written Old French, Written Middle French, Written Early Modern French, etc. (see TreeTagger software description for all available languages)",v0.8.3,,2023-11-07,,"Windows, MacOS, Linux",,Copyright Not Evaluated,"Fully open (GNU GPLv3)
Pricing: Free","Local application: installed by OS dependent installation software
"," GUI in three languages (FR, EN & RU), script in Groovy, script in R
","Import a set of documents as a corpus.
Convert from several input formats to XML-TEI TXM internal pivot format, available input formats: .docx, .odt, .rtf, .txt, .xml, .tei, .tmx, .xlsx, .ods.
Run annotation tools for automatic annotation: TreeTagger (production), UDPipe, Stanford NLP, Talismane... (prototypes).
Manually annotate or correct automatic annotations through the GUI.
Extract various textual observables based on lexical patterns for searching and counting (for statistical models).
Build various sub-corpora and partitions.
Build various visualisations and data tables from statistical analysis of observables inside sub-corpora/partitions.
","DOCX, ODT, RTF, TXT, XML, TEI, TMX, XLSX, ODS, R dataframes, BMP","As spreadsheets [results data], as R data frames [results data], as images (vector or bitmap) [data visualisations], as XML-TEI [texts and annotations]. 
All input formats are converted to XML-TEI TXM pivot format internaly, including any XML-TEI as input. The XML-TEI TXM format is a specific TEI extension that can be normalized to TEI on export.","This tool is not a statistical model / a set of statistical models, but it does include statistical models.
Yes, TreeTagger can be trained from the GUI to build new language models from a corpus.","TEI, TEI TXM","The tool imports a set of documents as a corpus.
Metadata can be associated with each document through a spreadsheet file.
The structure of documents depends on their type:
- Written texts [the structure model is TEI format] : any XML-TEI structure encoding, links to fac-simile pages images, hyperlinks, etc.
- Speech transcriptions (eg interview recording) [the structure model is Transcriber software file format]: link to media file, episodes, speech turns and word level timing with media files
- Multilingual/parallel [the structure model is TMX format]: alignments on any structure (div, p...)
- As a table (eg answers to a survey, tweets set) [the structure model is a spreadsheet]: some columns for metadata and some columns for textual content

All input formats are converted to XML-TEI TXM pivot format internaly, including any XML-TEI as input. The XML-TEI TXM format is a specific TEI extension that can be normalized to TEI on export.",udpipe,"Statistics expressing the mutual attraction of two words or short text segments (specificity statistical measure):
Statistics expressing the attraction of a word or a short text segment with a sub-corpus of the whole corpus or with parts of a partition of the corpus (specificity statistical measure).
khi2","Bar plot showing statistic value for each word pattern searched, clusters hierarchy, factorial planes","TIGER Search, CQL, URSQL. Unit-Relation-Schema (URS)",TreeTagger
10,TXM portal,,"A TXM portal hosts textual corpora for online access and analysis.
Users can create a user account, verify their mail address, allow authenticated of anonymous access to each corpus and access their corpora online with a simple web browser. No TXM software installation is needed. A similar GUI as TXM desktop is built in the browser to access and analyze corpora, an URL based API allows to start a TXM portal session in a specific command with parameters on a specific corpus. TXM web portals are often used for courses with students.
Depending on access permission settings (e.g., set access permissions to user profiles, allow or disallow access to each corpus or on text level by commands), access can be anonymous (by anybody) or need a connexion and specific rights. This allows to share corpora within working groups. 
TXM portals allows to host rich text editions and combine them with analytic tools. See below for some examples of direct access to text editions inside a corpus hosted in a TXM portal.
A TXM portal is managed by a special user called a ‘portal administrator’ who manages corpora, user accounts and access permissions.
Corpora are first created with the TXM desktop software, then exported in a .txm file and uploaded to a TXM portal.
See the documentation of the tool “TXM desktop” for further details on the kind of corpora that can be hosted and the tools provided for analysis.
A TXM portal doesn’t implement all the analysis tools of the TXM desktop software, but whose two tools share the same platform core and are developped together.
Examples of direct access to a TXM portal by URL API:
- Direct access to an Edition page of an Old French (1225) Holy Grail manuscript transcription highligthing words “dist” and “Lancelot” with a synoptic view of 'facsimile, diplomatic and critical edition': http://portal.textometrie.org/demo?command=edition&path=/GRAAL&textid=qgraal_cm&editions=ms-colonne,diplomatique,courante&wordids=w_106_030047,w_106_030049 
- Direct access to a Concordance of the word 'Lancelot' (then double-click on a line to read the full text): http://portal.textometrie.org/demo?command=concordance&path=/GRAAL&query=%22Lancelot%22 
- Direct access to an Edition page of an ancient Babylonian (-IIth mil.) tablet transcription with synoptic view combining the transliterated version, the cuneiform version and the facsimile image of the tablet: http://portal.textometrie.org/demo/?command=edition&path=/OBLCUNEIF&textid=TXM_cuneif_Ha_S_AbB_2_4&editions=translit,cuneiform,facs&pageid=3 

Examples of public TXM portals:
- Lyon, BFM-TXM portal (Old French litterature), UMR IHRIM laboratory: http://txm.bfm-corpus.org/ 
- Tours, BVH-TXM portal (Montaigne, Rabelais, Ronsard, etc.), UMR CESR laboratory: http://txm.bvh.univ-tours.fr/txm 
- Montpellier, Praxiling TXM portal, UMR PRAXILING laboratory: http://textometrie.univ-montp3.fr/txm 
- Paris, TXM portal of the French TreeBank (FTB) corpus, UMR LLF laboratory: http://manganese.lab.parisdescartes.fr:8813/txm/ 
- Besançon, TXM portal, EA ELLIADD laboratory: http://fanum-txm.univ-fcomte.fr/txm 


This tool is compatible with: TreeTagger, Corpus Query Processor, R
","portal management, corpus management, corpus analysis, information extraction",language-agnostic,"Like the TXM desktop, the tool is fully language independent: any Unicode script and character collation.
Linguistic annotations depend on NLP tools used or annotations imported from source.
For example, TreeTagger can lemmatize English, German, Italian, Spanish, Russian, Latin, Greek, Ancient Greek, written modern French, spoken modern French, written old French, writen middle French, written early modern French, etc. (see TreeTagger description for all available languages).
",v0.6.3,,2022-12-02,,Linux,Linux - Ubuntu 20.04,Copyright Not Evaluated,Fully free (GNU GPLv3),"Web application:
Administrators install the TXM portal software on a Ubuntu server.
Users access the TXM portal through a simple web browser (no installation).
","GUI, API
","No source text processing on a TXM portal.
Corpora are first created locally from sources with TXM desktop software, then exported to a .txm file, and then uploaded to a TXM portal.
","CSV, BMP","As .csv [results data], as images (bitmap) [data visualisations]. 
All formats are converted to XML-TEI TXM pivot format on input, including any XML-TEI as input. The XML-TEI TXM format is a specific TEI extension that can be normalized to TEI on export. If a corpus uploaded to a TXM portal contains the text pivot sources (option), then the corpus download from the TXM portail delivers the XML-TEI TXM text files","This tool is not a statistical model / a set of statistical models, and it does not include statistical models.","TEI, TEI TXM","The tool imports a set of documents as a corpus.
Metadata can be associated with each document through a spreadsheet file.
The structure of documents depend on their type:
- Written texts [structure model TEI]: any XML-TEI structure encoding, links to fac-simile images, hyperlinks, etc.
- Speech transcriptions (e. g. interview recording) [structure model Transcriber]: link to media file, episodes, speech turns and word level timing with media files
- Multilingual/parallel [structure model TMX]: alignments on any structure (div, p...)
- As a table (e. g. answers to a survey, tweets) [structure model spreadsheet]: columns for metadata and columns for textual content

All formats are converted to XML-TEI TXM pivot format on input, including any XML-TEI as input. The XML-TEI TXM format is a specific TEI extension that can be normalized to TEI on export. If a corpus uploaded to a TXM portal contains the text pivot sources (option), then the corpus download from the TXM portail delivers the XML-TEI TXM text files",,"Statistics expressing the mutual attraction of two words or short text segments (specificity statistical measure).
Statistics expressing the attraction of a word or a short text segment with a sub-corpus of the whole corpus or with parts of a partition of the corpus (specificity statistical measure).
chi-square",Bar plot showing statistic value for each word pattern searched,"CQL, TIGER Search",
11,Calc,,"Calc provides quick support to users when calculating basic statistical tasks most commonly encountered in corpus research. The GUI is divided into a number of modules reflecting specific research problems. Unlike other similar tools, Calc is task-based, which means that suitable statistical tests have already been pre-selected for a given task, so that users can draw statistically well-founded conclusions based on the input data.
The tool does not process a text. The input is typically numbers (frequencies) and the output is statistical significance, confidence intervals etc.","applying NLP methods, statistical analysis",language-agnostic,Any (it is language-agnostic),unknown,,unknown,,Linux,,Copyright Not Evaluated,fully free,Shiny web application,GUI,,,,"This tool is not a statistical model / a set of statistical models, and it does not include statistical models, but it implements basic statistical tests. ",,,,Various metrics,,,
12,Flair,,"Flair is an open-source Python library which allows users to 1) use the trained models provided by Flair and 2) to train their own NLP-models using the provided Python library framework. The tool builds upon the state-of-the-art deep learning Pytorch framework.
Examples of how a model can be trained can be found on their GitHub pages: https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md . 
Tasks you can perform include but are not limited to Named Entity Recognition (NER), sentiment analysis, Parts-of-speech tagging (POS) and text classification.
Since the tool is a Python library, you need a programming environment to use it. You can also use it in Colab, Google’s online IDE (example: https://www.analyticsvidhya.com/blog/2019/02/flair-nlp-library-python/). ",applying NLP methods,"Danish, Dutch, English, French, German, Spanish",,v0.11,,2022-04-10,,"Windows, Unix, MacOS, Linux",,Copyright Not Evaluated,Fully free ,Library for Python,none,"Automatically adds some labels (annotation) to your texts, whether it is a sentiment label on the level of a text chunk or an entity label on the sequence level.",TXT,"The tool does not support XML-TEI at all, only plain text.","The tool provides a framework to train your own Named Entity Recognition and sentiment analysis models, and to use the already trained Huggingface transformer models trained and made available by Flair.
You can easily train your own statistical model using the framework provided by this tool and your own labeled data.",TXT,"The tool accepts plain text as input to generate labels. To train models, it requires the data to be in a specific format as specified on their GitHub.",,,,Universal Dependencies,"Universal Dependencies. 
PER (person), LOC (location), ORG (organisation), MISC (miscellaneous).
positive, neutral, negative"
13,NameTag 1,,This tool marks named entities in your texts. You can easily supply to it your own NER model in any language. ,applying NLP methods,"Czech, English",This list presents currently available language models. Your own model can be for any language. ,v1.2.0,,2023-02,February 2023 (maintenance version of a release from 2021-04-21),Linux,,Mozilla Public License 2.0,Fully free (Mozilla Public License 2.0),"webservice (http://lindat.mff.cuni.cz/services/nametag).
code (https://github.com/ufal/nametag/releases/tag/v1.2.0). 
","web GUI, REST API",The tool detects and marks named entities.,TXT,The tool produces XML tags but does not support XML-TEI input. ,"This tool is not a statistical model / a set of statistical models, but it does include statistical models.
You can easily train your statistical model to plug in this tool. ","TXT, VERTICAL, CoNLL-U","plain text, vertical, CoNLL-U.
The tool produces XML tags but does not support XML-TEI input. ",NameTag1 Models,,,,depending on language model
14,NameTag 1 Models,,These are models to plug in the NameTag2 named-entity recognizer. ,applying NLP methods,"Czech, English",,unknown,,2014,,,,Creative Commons Attribution Non Commercial Share Alike 4.0 International,Fully free (CC-BY-NC-SA 4.0),zipped files from the LINDAT/CLARIAH-CZ repository,none,"It gives the named-entity recognizer the know-how of named-entity annotation in a given language, according to a given tagset. ",,,This tool is a statistical model / a set of statistical models.,,,,,,dependent on language model,dependent on language model
15,NameTag 2,,"This tool marks named entities in your texts. When named entities are nested, it recognizes the nestings. For instance, two entities can be recognized in Humboldt University: institution (the whole string) and surname (Humboldt). ",applying NLP methods,"Czech, Dutch, English, German, Spanish, Ukrainian","
This list presents currently available language models. ",v2.0.0,,2021-04-21,,Linux,,Mozilla Public License 2.0,Fully free (Mozilla Public License 2.0),"webservice (http://lindat.mff.cuni.cz/services/nametag).
code to run your own NameTag server.
commandline tool at (https://github.com/ufal/nametag/releases/tag/v2.0.0).
","web GUI, REST API",The tool detects and marks named entities.,TXT,The tool produces XML tags but does not support XML-TEI input. ,"This tool is (not) a statistical model / a set of statistical models, and/ but it does (not) include statistical models.
You can(not) easily train your statistical model to plug in this tool. ","TXT, VERTICAL, CoNLL-U","plain text, vertical, CoNLL-U.
The tool produces XML tags but does not support XML-TEI input. ",NameTag2 Models,,,,depending on language model
16,NameTag 2 Models,,These are models to plug in the NameTag2 named-entity recognizer. ,applying NLP methods,"Czech, Dutch, English, German, Spanish, Ukrainian",,unknown,,unknown,,,,Creative Commons Attribution Non Commercial Share Alike 4.0 International,Fully free (CC-BY-NC-SA 4.0),zipped files from the LINDAT/CLARIAH-CZ repository,none,"It gives the named-entity recognizer the know-how of named-entity annotation in a given language, according to a given tagset. ",,,This tool is a statistical model / a set of statistical models.,,,,,,dependent on language model,dependent on language model
17,QuitaUp,,"After you upload a text, the tool performs basic text-processing functions using UD Pipe (tokenization, lemmatization, POS tagging, syntactic parsing). These functions are available only for ca 20 languages. The tool then calculates ca 16 stylometric indices of the input text.","applying NLP methods, applying textometry methods",language-agnostic,The tool supports all languages that can be parsed with UDPipe. ,unknown,,unknown,,Linux,,Copyright Not Evaluated,Fully free ,Shiny web application,GUI,Returns some metrics as numbers or tables.,,,"This tool is not a statistical model / a set of statistical models, and it does not include statistical models.
You can(not) easily train your statistical model to plug in this tool. ","TXT, DOCX, DOC, ODT, RTF, PDF","The tool can handle .txt, .docx, .doc, .odt, .rtf or .pdf formats and treats them all as plain text. ",udpipe,"Token frequencies, Lexical richness/diversity, Key words, Thematic concentration, Descriptivity/activity, Verb distance, Entropy, Hapaxes",,,
18,spaCy,,"spaCy is a free open-source Python library for advanced Natural Language Processing purposes.
Everything the library can do is neatly outlined here: https://spacy.io/usage/spacy-101/ . 
Tasks you can perform include but are not limited to Named Entity Recognition (NER), sentiment analysis, Parts-of-speech tagging (POS), lemmatization, entity linking, rule-based matching, and text classification. It allows you to train your own domain-specific models – but you can also use the ones provided by the library.
Since the tool is a Python library, you need a programming environment to use it, and knowledge of NLP. For digital humanities purposes, there is a great YouTube series (https://www.youtube.com/@python-programming) which shows how you can use spaCy in different DH settings (from information extraction to visualization).",applying NLP methods,"Catalan, Valencian, Chinese, Croatian, Danish, Dutch, English, Finnish, French, German, Greek, Italian, Japanese, Korean, Lithuanian, Macedonian, Norwegian Bokmål, Polish, Portuguese, Romanian, Russian, Spanish, Swedish, Ukrainian",Dutch (not Flemish),v3.5,,2022-04-10,,"Windows, Unix, MacOS, Linux",,Copyright Not Evaluated,Fully free ,Library for Python,none,Automatically adds some labels (annotation) to your texts.,,,"This tool is not a statistical model / a set of statistical models, but it does include statistical models.
Yes. (see explanation here: https://spacy.io/usage/spacy-101/ ).",TXT,"It accepts plain text as input to generate labels. To train models, it requires the data to be in a specific format as specified on their website (https://spacy.io/).",,,,Universal Dependencies,"Universal Dependencies.
positive, neutral, negative"
19,UDapi,,"Udapi is a framework providing an API for processing Universal Dependencies data. You can use it to similar tasks as the tree query languages PMLTQ or Grew, but it gives you much more freedom. Also, you can run Udapi on your data directly, while both PMLTQ and Grew need to first create a relational database of your entire data. This also means that you can process larger data with Udapi than with PMLTQ and Grew. 
However, this comes at a cost – you must be able to write a Python script and read developer documentation. So far, the tools has been used in the treebanking community, but there is no fool-proof tutorial yet for newcomers to the field. 
The tool is structured in packages and modules (https://udapi.readthedocs.io/en/latest/ modules.html). To search trees, the udapi.core package is the relevant one. Nodes and their attributes are represented as object classes with defined properties and methods. To explain the logic of these properties and methods, tutorials tailored to the CLS community are likely to be needed. ",applying NLP methods,language-agnostic,Any (it is language-agnostic),v0.3.0,,2022-04-06,,"Windows, MacOS, Unix, Linux",,Copyright Not Evaluated,Fully free (GNU GPL3.0),Python library,none,"The tool reads connl-u files and enables search, visualization as well as extraction of text/annotation snippets and tree manipulation.",,,"This tool is not a statistical model / a set of statistical models, and it does not include statistical models.",CoNLL-U,,,user can compute their own statistics,,Universal Dependencies,Universal Dependencies
20,UDPipe1,,"This is an older version of UDPipe2, the tagger, lemmatizer, and syntactic parser. It uses language models trained on the Universal Dependencies treebank collection, which you can browse and learn more about at universaldependencies.org. The language models are retrained twice a year, after the regular updates of the treebank collection by the Universal Dependencies Treebanks team. Depending on the changes in the treebank collection, the update may provide more training data for already existing language models or even new language models.
UDPipe1 achieves a somewhat lower performance than UDPipe2, but it is comfortably trainable by the users, given annotated data in the CoNLL-U format. It is still being maintained.  
This tool is compatible with Universal Dependencies Language Models",applying NLP methods,"Afrikaans, Ancient Greek, Arabic, Armenian, Basque, Belarusian, Bulgarian, Catalan, Chinese, Classical Chinese, Coptic, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Gothic, Greek, Hebrew, Hindi, Hungarian, Indonesian, Irish, Italian, Japanese, Korean, Latin, Latvian, Lithuanian, Maltese, Marathi, North Sami, Norwegian, Old Church Slavonic, Old French, Old Russian, Persian, Polish, Portuguese, Romanian, Russian, Scottish Gaelic, Serbian, Slovak, Slovenian, Spanish, Swedish, Tamil, Telugu, Turkish, Ukrainian, Urdu, Uyghur, Vietnamese, Wolof","Note that UDPipe 1 models differ from UDPipe2 models. The last pre-trained model version is 2.5 and contains fewer languages than more recent models. Also, some training corpora for the represented languages may be smaller.  
The parsers itself is language-agnostic. The list shows the pre-trained models. ",v1.3.0,,2023-02-16,,"Linux, Windows, MacOS",MacOS - OsX,Mozilla Public License 2.0,Fully free (Mozilla Public License 2.0),"UDPipe is available as a binary for Linux/Windows/OS X, and as a library for C++, Python, Perl, Java, C#. Third-party R CRAN package also exists. 

",,"It recognizes tokens and adds labels to each token, regarding its morphological characteristics as well as its syntactic relation to a governing token in the Universal Dependencies formalism. ",,,"This tool is not a statistical model / a set of statistical models, but it does include statistical models.
You can easily train your statistical model to plug in this tool. 
","TXT, CoNLL-U, VERTICAL","Plain text, CoNLL-U, vertical... see documentation for more detail on accepted plain-text formats. ",,,,Universal Dependencies,Universal Dependencies
21,UDPipe2,,"This is a tagger, lemmatizer, and syntactic parser. It uses language models trained on the Universal Dependencies treebank collection, which you can browse and learn more about at universaldependencies.org. The language models are retrained twice a year, after the regular updates of the treebank collection by the Universal Dependencies Treebanks team. Depending on the changes in the treebank collection, the update may provide more training data for already existing language models or even new language models. 
UDPipe is a state-of-the art system. This means that it is able to learn from less data than older systems. However, the truth is, still, that the quality of the analysis depends on the size of training data related to the complexity of the language. Hence, e. g. languages with rich inflection or very specific syntax need bigger training data. Besides, the UDPipe developers do normally not actively look for external resources for specific languages, such as morphological lexicons. There are known cases of parsers optimized for one given language with bigger data and additional resources, which perform better than UDPipe, e. g. the Magyarlanc parser for Hungarian. 
When considering several parsers for your language, have a look at the evaluation of the UDPipe performance with the models from a given relase here: https://ufal.mff.cuni.cz/udpipe/2/models
When you evaluate this parser for your data, mind to record the model and version you were using. Note that one language can have several separate models from different domains, e. g.  newspapers and data from social networks!
This tool is compatible with Universal Dependencies Language Models, UDPipe 1 (it uses its tokenizer by default).",applying NLP methods,"Afrikaans, Ancient Greek, Ancient Hebrew, Arabic, Armenian, Basque, Belarusian, Bulgarian, Catalan, Chinese, Classical Chinese, Coptic, Croatian, Czech, Danish, Dutch, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Korean, Latin, Latvian, Lithuanian, Maghrebi Arabic French, Maltese, Manx, Marathi, Naija, North Sami, Norwegian, Old Church Slavonic, Old East Slavic, Old French, Persian, Polish, Pomak, Portuguese, Romanian, Russian, Sanskrit, Scottish Gaelic, Serbian, Slovak, Slovenian, Spanish, Swedish, Tamil, Telugu, Turkish, Ukrainian, Urdu, Uyghur, Vietnamese, Welsh, Western Armenian, Wolof","The parser itself is language-agnostic, but it works with language-specific models. The current models exist for languages listed in the table below. ",unknown,"Note: UDPipe exists in two versions. Version 2 is a Python prototype (while UDPipe 1 is a user-friendly standalone C++ application with bindings for Python, Java, C#, and Perl). In general, it is meant for research, not as a user-friendly replacement for UDPipe1. 
It does not perform tokenization by itself: it uses UDPipe1 to do that, or you can select a language-independent generic script. In comparison to UDPipe1, its language models require more computation power (while the UDPipe1 models are small and fast). On the other hand, it mostly performs better than UDPipe1, since the models are neural-network (BERT) based. 
If you use the R library udpipe by J. Wijffels, mind that it operates with UDPipe 1. If you want to take advantage of the UDPipe2 output, you have to call the UDPipe2 API outside of this library (and then you can of course use the resulting CoNLL-U file with other udpipe functions).    ",2022-08-05,,Linux,"Linux (not tested for other operation systems).
Online through a REST API.
",Creative Commons Attribution Non Commercial Share Alike 4.0 International,License: Fully free (CC BY-NC-SA 4.0),"Local application (source code at https://github.com/ufal/udpipe/releases/tag/v2.0.0).
Cloud service.
Python script to interact with the API (https://github.com/ufal/udpipe/blob/udpipe-2/udpipe2_client.py).",,Automatically adds some labels (annotation) to your texts.,CoNLL-U,UDPipe 2 does not support XML-TEI at all but there are various tools to combine the CoNLL-U output with TEI.,"This tool is not a statistical model / a set of statistical models, but it does include statistical models.
To train a model for UDPipe2 yourself, you would have to set up a UDPipe 2 instance locally and run the source scripts as described here: https://github.com/ufal/udpipe/tree/udpipe-2.   
So, theoretically yes, but it is not meant to be performed by users without solid programming background.  
If you want to train your model yourself before the release for UDPipe 2, you can easily do so with UDPipe 1, although probably with somewhat lower performance than UDPipe2 would achieve.
You are very welcome to contribute to the Universal Dependencies Treebanks. Once your treebank is valid and large enough (10,000 tokens), it will be automatically included in the next version of the models for UDPipe 2. ","CoNLL-U, VERTICAL","The input for UDPipe 2 parser must be UTF-encoded text without any other markup. Structuring options:
- CoNLL-U
- horizontal (each sentence on a separate line)
- vertical (each token on a separate line, with an empty line denoting end of the sentence.  
For details, see https://lindat.mff.cuni.cz/services/udpipe/api-reference.php and look for Method process, parameter input.

",UDPipe1,,,Universal Dependencies,Universal Dependencies
22,Universal Dependencies Models for UDPipe,UDPipe language models,"This is a package of language models for the UDPipe parser and tagger. It is updated after each update of the Universal Dependencies treebank collection at universaldependencies.org. Depending on the changes in the treebank collection, the update may provide more training data for already existing language models or even new language models. 
UDPipe is a state-of-the art system. This means that it is able to learn from less data than older systems. However, the truth is, still, that the quality of the analysis depends on the size of training data related to the complexity of the language. Hence, e. g. languages with rich inflection or very specific syntax need bigger training data. Besides, the UDPipe developers do normally not actively look for external resources for specific languages, such as morphological lexicons. There are known cases of parsers optimized for one given language with bigger data and additional resources, which perform better than UDPipe, e. g. the Magyarlanc parser for Hungarian. 
When considering several parsers for your language, have a look at the evaluation of the UDPipe performance with the models from a given relase here: https://ufal.mff.cuni.cz/udpipe/2/models
This tool is integrated into the UDPipe2 parser. ",applying NLP methods,,,v2.10.,"These models form an eco-system with Universal Dependencies treebanks (Agić et al., 2015). The Universal Dependencies Treebanks are regularly updated twice a year, and so are the models based on them
",2022-07-11,,"Windows, Unix, MacOS, Linux",,Creative Commons Attribution Non Commercial Share Alike 4.0 International,Fully free (CC BY-NC-SA 4.0),a zipped file ,none,"When plugged in the UDPipe parser, it makes it automatically add some labels (annotation) to your texts.",,,"This tool is a statistical model / a set of statistical models.
You cannot easily train your statistical model to plug in this tool. It is possible, but rather complex for a user without solid programming background. However, you are very welcome to contribute to the Universal Dependencies Treebanks. Once your treebank is valid and large enough (min. 10,000 tokens), it will be automatically included in the next version of the models for the current UDPipe parser (UDPipe 2.0). 
If you want to train your model yourself before the release for UDPipe2, you can easily do so with UDPipe 1.0, with probably somewhat lower performance than UDPipe2 would achieve.
To train a model for UDPipe2 yourself, you would have to set up a UDPipe instance locally and run the source scripts as described here: https://github.com/ufal/udpipe/tree/udpipe-2.   
",,,UDPipe2,,,Universal Dependencies,Universal Dependencies
23,udpipe,UDPipe R library,"This tool is an R library that allows for using the UDPipe 1 parser directly within R. Besides, it provides a range of handy functions for text mining, such as basic frequency statistics, collocation analysis, key word extraction, and topic modeling with Latent Dirichlet Allocation as well as Latent Semantic Analysis.
This tool is compatible with UDPipe1, UDPipe Language Models.","applying NLP methods, corpus analysis",language-agnostic,"Note that this tool uses the UDPipe 1 parser and hence can only use models up to version 2.5, which is the last pre-trained model version for UDPipe1. It contains fewer languages than more recent models trained for UDPipe2, which you cannot use through this library. Also, some training corpora for the represented languages may be smaller.  
The parsers itself is language-agnostic. The list below shows the pre-trained models",v0.8.11,,2023-01-06,,"Windows, Unix, MacOS, Linux",,Mozilla Public License 2.0,Fully free (MPL-2.0),,none,"It runs the UDPipe 1 parser on your text. In addition, you can analyze keywords, collocations, and topics of any text in the CoNLL-U format. That is, you can parse something with UDPipe 2 outside this library directly through the UDPipe2 REST API (e.g. using the RCurl library in R) and then continue with this udpipe library to analyze the CoNLL-U file. ",,,"This tool is not a statistical model / a set of statistical models, but it does include statistical models.
You can easily train your statistical model to plug in this tool. ",TXT,Plain text,UDPipe1,Probabilities of different tags with different words for a language (domain),,Universal Dependencies,Universal Dependencies
24,Alberti,Alberti Multilingual Model,"This is a language model trained with poetic texts that returns word embeddings. It has been trained for the word masking task, but it fills the masked token by trying ot be poetic rather than the most probable one. For example, for the sentence “I went to the library to buy a <MASK>” it would return “treasure” instead of “book”.",poetry processing,"English, French, Italian, Czech, Portugese, Spanish, Arabic, Chinese, Finnish, German, Hungarian, Russian",,v1.0,,2022,,"Windows, Unix, MacOS",,Creative Commons Attribution 4.0 International,Fully free (CC-BY-4.0),Language model (transformers),none,It returns word-embeddings,,,"This tool is a statistical model / a set of statistical models (transformer).
You can easily train your statistical model to plug in this tool. ",,,,,,,
25,Alberti-stanzas,,This is a language model trained with poetic texts that returns word embeddings. It has been trained for the stanzas detection task port Spanish poetry. It can return the stanza type of a Spanish poem.,"poetry processing, applying NLP methods",Spanish,,unknown,,2022,,"Windows, Unix, MacOS, Linux",,Creative Commons Attribution 4.0 International,CC-BY-4.0,Language model (transformers),GUI,Automatic annotation of stanza type for Spanish poems,,,"It is a fine-tuned transformers model.
You can easily train your statistical model to plug in this tool. ",TXT,Plain text,,,,,
26,Averell,,"Averell is a python library and command line interface that facilitates working with existing repositories of annotated poetry. Averell is able to download an annotated corpus and reconcile different TEI entities to provide a unified JSON output at the desired granularity. That is, for their investigations some researchers might need the entire poem, poems split line by line, or even word by word if that is available. Averell allows to specify the granularity of the final generated dataset, which is a combined JSON with all the entities in the selected corpora. Each corpus in the catalog must specify the parser to produce the expected data format.","poetry processing, applying NLP methods",language-agnostic,"For automatic annotation: Spanish / For automatic integration: any (it is language-agnostic)
",v1.2.3,,2023-12-11,,"Windows, Unix, MacOS, Linux",,Apache Software License 2.0,"Fully free (Apache Software License 2.0)

","Local application, Library for Python
","GUI, Command line","Automatically adds some labels (annotation) to your texts.
Downloads an annotated corpus and reconcile different TEI entities to provide a unified JSON output at the desired granularity.
","JSON, XML RDF, TEI","JSON,  Ontopoetry RDF, TEI work in progress.
The tool requires or accepts text in XML-TEI, adds its markup in XML-TEI, and ensures valid XML-TEI on the output. 
The tool requires or accepts text in XML-TEI but the original markup gets lost and/or the output will not be integrated into the original XML-TEI format.    ","This tool is not a statistical model / a set of statistical models, and it does not include statistical models.",TEI,"The input format does not matter but the following information must be given as a bare minimum:
- Corpus name
- Annotation type (manual, automatic, or none)
- Author name
- Poem title
- Poem text split into stanzas

The tool requires or accepts text in XML-TEI, adds its markup in XML-TEI, and ensures valid XML-TEI on the output. 
The tool requires or accepts text in XML-TEI but the original markup gets lost and/or the output will not be integrated into the original XML-TEI format.    ",Rantanplan,,,,
27,Horace,,"Format converter from PoetryLab JSON to POSTDATA semantic formats. It receives a scansion dictionary from Rantanplan and outputs an RDFLib  Graph object. With it, serialization options become available.","poetry processing, applying NLP methods",Spanish,,unknown,,2021,,"Windows, MacOS, Unix",,Apache Software License 2.0,Fully free (Apache 2.0),"Local application, Library for Python
",none,"Automatically adds some labels (annotation) to your texts.

",,,"This tool is not a statistical model / a set of statistical models, and it does not include statistical models.",JSON,JSON using the rantanplan format,,,,,
28,Poetrylab,alias Poetrylab web,"This is a web application show and visualize poetic information for the POSTDATA multilingual corpus. You can search by author, or poem name, or write your own poem, and you can visualize the metrical and structural information.","poetry processing, automatic annotation, corpus management",language-agnostic,This tool is language-agnostic (supports any language),unknown,,2022-04-27,,"Windows, Unix, MacOS",,Apache Software License 2.0,Fully free (Apache 2.0),Local application,GUI,"Automatically adds some labels (annotation) to your texts.
Allows you to search through your texts and returns matching results.",,,"This tool is not a statistical model / a set of statistical models, and it does not include statistical models.",TXT,Plain text,,,,,
29,Poetrylab-API,,"The PoetryLab API is a REST API that provides an analysis  endpoint to extract information about the poems. It can annotate Spanish poetry with rhyme, metric, enjambment, and part of speech.
Only the method POST is allowed at the /analysis endpoint. A playground interface documenting the API can be found at the /ui endpoint.","poetry processing, applying NLP methods",Spanish,,unknown,,2021-09-17,,"Windows, Unix, MacOS",,Apache Software License 2.0,Fully free (Apache 2.0),"REST API, Library for Python
",API-REST,The tool automatically adds some labels (annotation) to your texts.,,,"This tool is a statistical model / a set of statistical models, but it does not include statistical models.
You can easily train your statistical model to plug in this tool. ",TXT,Plain text,Rantanplan,,,,
30,rhymetagger,,"A simple collocation-driven recognition of rhymes. Contains pre-trained models for Czech, Dutch, English, French, German, Russian, and Spanish poetry.
The tool accepts a flat Python list of verse lines or list of lists (stanzas X lines) as input.","poetry processing, applying NLP methods",language-agnostic,"The tool is language-agnostic (i. e. supports any language you train your language model for). Currently it has pre-trained models for Czech, Dutch, English, French, German, Russian, and Spanish.   ",v0.2.2,,2021-03-02,,"Windows, Unix, MacOS, Linux",,Copyright Not Evaluated,Fully free,Library for Python,none,Returns some metrics as numbers or tables.,,,"This tool is not a statistical model / a set of statistical models, but it does not include statistical models.
You can easily train your statistical model to plug in this tool. ",,,,,,,
31,Rantanplan,,"It can annotate Spanish poetry with rhyme, meter, and part of speech.","poetry processing, applying NLP methods",Spanish,,v0.8.0,,2023-12-19,,"Windows, Unix, MacOS, Linux",,Apache Software License 2.0,,Library for Python,none,Automatically adds some labels (annotation) to your texts.,,,"This tool is not a statistical model / a set of statistical models, and it does not include statistical models.
You can easily train your statistical model to plug in this tool. ",TXT,Plain text,,,,,